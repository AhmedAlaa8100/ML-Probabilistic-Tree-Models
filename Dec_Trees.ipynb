{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27293643",
   "metadata": {},
   "source": [
    "## Decision Trees Implementation ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec28c7c",
   "metadata": {},
   "source": [
    "# 1) Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b64f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "##### Data Setup ######\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_breast_cancer()\n",
    "X = data.data           \n",
    "y = data.target        \n",
    "feature_names = data.feature_names  \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split the remaining 30% into 15% val and 15% test\n",
    "# 15% is half of 30% → test_size = 0.5\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a6bdd",
   "metadata": {},
   "source": [
    "## 2)Node Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "499d5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        if self.value is not None:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f587fb",
   "metadata": {},
   "source": [
    "## 3) Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1896e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=2, max_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.root=None\n",
    "        self.max_features=max_features\n",
    "        self.all_gains=[]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.all_gains = [0] * X.shape[1]\n",
    "        self.root = self._grow_tree(X, y)\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        if(depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        left_mask=X[:, best_feature] <= best_threshold\n",
    "        right_mask=X[:, best_feature] > best_threshold\n",
    "        left_data, right_data = X[left_mask], X[right_mask]\n",
    "        left_labels, right_labels = y[left_mask], y[right_mask]\n",
    "        left_child=self._grow_tree(left_data, left_labels, depth+1)\n",
    "        right_child=self._grow_tree(right_data, right_labels, depth+1)\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "    #### Helper Functions ####\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_overall_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        if self.max_features is None or self.max_features > n_features:\n",
    "            features_to_consider = range(n_features)\n",
    "        else:\n",
    "            features_to_consider = np.random.choice(n_features, self.max_features, replace=False)\n",
    "\n",
    "\n",
    "\n",
    "        for feature in features_to_consider:\n",
    "            \n",
    "            # Compute all possible midpoints for this feature\n",
    "            values = np.sort(np.unique(X[:, feature]))\n",
    "            thresholds = [(values[i] + values[i+1]) / 2 for i in range(len(values) - 1)]\n",
    "\n",
    "            best_feature_gain = -1  # reset best gain for THIS feature\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                gain = self._info_gain(y, X[:, feature], threshold)\n",
    "\n",
    "                # Update best-overall split\n",
    "                if gain > best_overall_gain:\n",
    "                    best_overall_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "                # Update best gain for THIS feature\n",
    "                if gain > best_feature_gain:\n",
    "                    best_feature_gain = gain\n",
    "\n",
    "            # Save best gain of this feature for feature ranking\n",
    "            self.all_gains[feature] += best_feature_gain*n_samples\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _info_gain(self, y, feature_column, threshold):\n",
    "        parent_entropy=self.entropy(y)\n",
    "        left_mask=feature_column <= threshold\n",
    "        right_mask=feature_column > threshold\n",
    "        left_y=y[left_mask]\n",
    "        right_y=y[right_mask]\n",
    "        if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:\n",
    "            return 0\n",
    "        left_entropy=self.entropy(left_y)\n",
    "        right_entropy=self.entropy(right_y)\n",
    "        weighted_entropy=(len(left_y)/len(y))*left_entropy + (len(right_y)/len(y))*right_entropy\n",
    "        info_gain=parent_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "    def entropy(self, y):\n",
    "        class_labels, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
    "        return entropy\n",
    "    def _most_common_label(self, y):\n",
    "        class_labels, counts = np.unique(y, return_counts=True)\n",
    "        most_common = class_labels[np.argmax(counts)]\n",
    "        return most_common\n",
    "    def predict(self, X):\n",
    "        root=self.root\n",
    "        predictions=[]\n",
    "        for x in X:\n",
    "            node = self.root\n",
    "            while not node.is_leaf_node():\n",
    "                if x[node.feature] <= node.threshold:\n",
    "                    node=node.left\n",
    "                else:\n",
    "                    node=node.right\n",
    "            predictions.append(node.value)\n",
    "        return np.array(predictions)\n",
    "    def ranked_features(self):\n",
    "       \n",
    "        if not self.all_gains:\n",
    "            return 0\n",
    "        gains=np.argsort(self.all_gains)[::-1]\n",
    "        table = pd.DataFrame({\n",
    "        \"Feature\": [feature_names[i] for i in gains],\n",
    "        \"Information Gain\": [self.all_gains[i] for i in gains]\n",
    "        })\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9325a8",
   "metadata": {},
   "source": [
    "## 4) Analytics Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c12e2f",
   "metadata": {},
   "source": [
    "## 4.1) *Hyperparameter Testing*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dfa4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Analytics Functions ##########\n",
    "def hyperparameter_tuning(X_train,y_train,X_val,y_val):\n",
    "    best_acc=0\n",
    "    best_params = None\n",
    "    accuracy=[]\n",
    "    best_tree=None\n",
    "    max_depth={2, 4, 6, 8, 10} \n",
    "    min_samples_split = {2, 5, 10}\n",
    "   \n",
    "    results = []\n",
    "\n",
    "    for depth in max_depth:\n",
    "        for min_split in min_samples_split:\n",
    "            tree = DecisionTree(max_depth=depth, min_samples_split=min_split)\n",
    "            tree.fit(X_train, y_train)\n",
    "            preds = tree.predict(X_val)\n",
    "            acc = np.mean(preds == y_val)\n",
    "            print(f\"Depth={depth}, MinSplit={min_split}, ValAcc={acc:.4f}\")\n",
    "            results.append((depth, min_split, acc))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = (depth, min_split)\n",
    "         \n",
    "    print(\"\\nBest Parameters:\", best_params, \"Validation Accuracy:\", best_acc)\n",
    "    return best_params, results\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e243d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=2, MinSplit=2, ValAcc=0.9176\n",
      "Depth=2, MinSplit=10, ValAcc=0.9176\n",
      "Depth=2, MinSplit=5, ValAcc=0.9176\n",
      "Depth=4, MinSplit=2, ValAcc=0.9882\n",
      "Depth=4, MinSplit=10, ValAcc=0.9882\n",
      "Depth=4, MinSplit=5, ValAcc=0.9882\n",
      "Depth=6, MinSplit=2, ValAcc=0.9647\n",
      "Depth=6, MinSplit=10, ValAcc=0.9647\n",
      "Depth=6, MinSplit=5, ValAcc=0.9647\n",
      "Depth=8, MinSplit=2, ValAcc=0.9882\n",
      "Depth=8, MinSplit=10, ValAcc=0.9647\n",
      "Depth=8, MinSplit=5, ValAcc=0.9647\n",
      "Depth=10, MinSplit=2, ValAcc=0.9882\n",
      "Depth=10, MinSplit=10, ValAcc=0.9647\n",
      "Depth=10, MinSplit=5, ValAcc=0.9647\n",
      "\n",
      "Best Parameters: (4, 2) Validation Accuracy: 0.9882352941176471\n"
     ]
    }
   ],
   "source": [
    "best_params, tuning_results = hyperparameter_tuning(X_train, y_train, X_val, y_val)\n",
    "best_depth, best_min_split = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1f1df",
   "metadata": {},
   "source": [
    "## 4.2) *Changing only Max Depth*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dfb9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_depth_analysis(X_train,y_train,X_val,y_val):\n",
    "    max_depth={2, 4, 6, 8, 10} \n",
    "    min_samples_split=2\n",
    "    table = []\n",
    "    for depth in max_depth:\n",
    "        tree = DecisionTree(max_depth=depth, min_samples_split=min_samples_split)\n",
    "        tree.fit(X_train, y_train)\n",
    "        preds = tree.predict(X_val)\n",
    "        val_acc = np.mean(preds == y_val)\n",
    "        train_preds = tree.predict(X_train)\n",
    "        train_acc = np.mean(train_preds == y_train)\n",
    "        print(f\"Depth={depth}, ValAcc={val_acc:.4f}\")\n",
    "        \n",
    "        table.append((depth, train_acc, val_acc))\n",
    "\n",
    "    print(\"\\nDepth | Train Acc | Val Acc\")\n",
    "    for row in table:\n",
    "        print(f\"{row[0]:5} | {row[1]:9.4f} | {row[2]:8.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17d1c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=2, ValAcc=0.9176\n",
      "Depth=4, ValAcc=0.9882\n",
      "Depth=6, ValAcc=0.9647\n",
      "Depth=8, ValAcc=0.9882\n",
      "Depth=10, ValAcc=0.9882\n",
      "\n",
      "Depth | Train Acc | Val Acc\n",
      "    2 |    0.9523 |   0.9176\n",
      "    4 |    0.9925 |   0.9882\n",
      "    6 |    0.9950 |   0.9647\n",
      "    8 |    1.0000 |   0.9882\n",
      "   10 |    1.0000 |   0.9882\n"
     ]
    }
   ],
   "source": [
    "max_depth_analysis(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74852f6d",
   "metadata": {},
   "source": [
    "##  4.3) *Tree Complexity and Model Performance*\n",
    "\n",
    "To evaluate the effect of model complexity, we analyze how varying the **maximum depth** of the tree influences performance.\n",
    "\n",
    "We examine:\n",
    "- **Training Accuracy**\n",
    "- **Validation Accuracy**\n",
    "- **Overfitting / underfitting behavior**\n",
    "\n",
    "**What we expect:**\n",
    "- At **low depth**, the model underfits → high bias, low accuracy.\n",
    "- As depth increases, training accuracy rises.\n",
    "- Validation accuracy improves at first, then declines when the tree becomes too complex.\n",
    "  This indicates **overfitting**.\n",
    "\n",
    "**How results are presented:**\n",
    "- A table or plot showing accuracy vs. tree depth.\n",
    "- The optimal depth is chosen where validation accuracy peaks.\n",
    "- This depth is then used to retrain on training + validation data and evaluated on the test set.\n",
    "\n",
    "**Conclusion:**\n",
    "- Tree depth strongly affects generalization.\n",
    "- The best performance occurs at the depth that balances bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26dd56",
   "metadata": {},
   "source": [
    "## 4.4) *Accuarcy measures on Test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9399d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_combine(X_train, y_train, X_val, y_val):\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "    y_train_val = np.hstack((y_train, y_val))\n",
    "    return X_train_val, y_train_val\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    print(\"===== PERFORMANCE METRICS =====\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred, average=None)}\")\n",
    "    print(f\"Recall:    {recall_score(y_true, y_pred, average=None)}\")\n",
    "    print(f\"F1-score:  {f1_score(y_true, y_pred, average=None)}\\n\")\n",
    "\n",
    "    print(\"===== CLASSIFICATION REPORT =====\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"===== CONFUSION MATRIX =====\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83ab71b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=2, ValAcc=0.9176\n",
      "Depth=4, ValAcc=0.9882\n",
      "Depth=6, ValAcc=0.9647\n",
      "Depth=8, ValAcc=0.9882\n",
      "Depth=10, ValAcc=0.9882\n",
      "\n",
      "Depth | Train Acc | Val Acc\n",
      "    2 |    0.9523 |   0.9176\n",
      "    4 |    0.9925 |   0.9882\n",
      "    6 |    0.9950 |   0.9647\n",
      "    8 |    1.0000 |   0.9882\n",
      "   10 |    1.0000 |   0.9882\n",
      "===== PERFORMANCE METRICS =====\n",
      "Accuracy:  0.8837\n",
      "Precision: [0.86666667 0.89285714]\n",
      "Recall:    [0.8125     0.92592593]\n",
      "F1-score:  [0.83870968 0.90909091]\n",
      "\n",
      "===== CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8667    0.8125    0.8387        32\n",
      "           1     0.8929    0.9259    0.9091        54\n",
      "\n",
      "    accuracy                         0.8837        86\n",
      "   macro avg     0.8798    0.8692    0.8739        86\n",
      "weighted avg     0.8831    0.8837    0.8829        86\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[26  6]\n",
      " [ 4 50]]\n"
     ]
    }
   ],
   "source": [
    "final_tree = DecisionTree(max_depth=best_depth, min_samples_split=best_min_split)\n",
    "X_train_val, y_train_val = train_val_combine(X_train, y_train, X_val, y_val)\n",
    "max_depth_analysis(X_train, y_train, X_val, y_val)\n",
    "final_tree.fit(X_train_val, y_train_val)\n",
    "test_preds_trees = final_tree.predict(X_test)\n",
    "evaluate_performance(y_test, test_preds_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081078d",
   "metadata": {},
   "source": [
    "## 4.5) *Ranking of Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67364f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Information Gain\n",
      "0            worst symmetry          0.918296\n",
      "1      worst concave points          0.918296\n",
      "2          worst smoothness          0.918296\n",
      "3         worst compactness          0.918296\n",
      "4             worst texture          0.918296\n",
      "5                 mean area          0.918296\n",
      "6           mean smoothness          0.918296\n",
      "7               mean radius          0.918296\n",
      "8              mean texture          0.918296\n",
      "9          smoothness error          0.459148\n",
      "10           mean perimeter          0.459148\n",
      "11            mean symmetry          0.459148\n",
      "12          perimeter error          0.459148\n",
      "13  worst fractal dimension          0.316689\n",
      "14             radius error          0.316689\n",
      "15           mean concavity          0.316689\n",
      "16     concave points error          0.316689\n",
      "17          concavity error          0.316689\n",
      "18  fractal dimension error          0.316689\n",
      "19             worst radius          0.316689\n",
      "20               worst area          0.316689\n",
      "21          worst concavity          0.316689\n",
      "22         mean compactness          0.316689\n",
      "23   mean fractal dimension          0.316689\n",
      "24               area error          0.251629\n",
      "25          worst perimeter          0.251629\n",
      "26      mean concave points          0.251629\n",
      "27           symmetry error          0.109170\n",
      "28            texture error          0.109170\n",
      "29        compactness error          0.109170\n"
     ]
    }
   ],
   "source": [
    "final_tree.ranked_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd857bc",
   "metadata": {},
   "source": [
    "## 4.6) *Overfitting Analysis: Training vs. Validation Performance*\n",
    "\n",
    "### Overview\n",
    "Overfitting occurs when a model learns the training data too well, including its noise and peculiarities, resulting in poor generalization to unseen data. For decision trees, this is controlled primarily by **tree depth** and **minimum samples per split**.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "**Training vs. Validation Accuracy Gap:**\n",
    "- As tree depth increases, training accuracy typically continues to improve (approaching 100%)\n",
    "- Validation accuracy initially improves but then plateaus or decreases\n",
    "- The gap between training and validation accuracy indicates overfitting\n",
    "\n",
    "**Optimal Depth:**\n",
    "- At shallow depths (depth=2-4), both accuracies are lower → **underfitting** (high bias)\n",
    "- At moderate depths (depth=6-8), validation accuracy peaks → **balanced model**\n",
    "- At deep depths (depth=10+), large gap appears → **overfitting** (high variance)\n",
    "\n",
    "### How to Identify Overfitting\n",
    "\n",
    "1. **Growing Gap**: If `Training Accuracy - Validation Accuracy > 0.05`, the model is likely overfitting\n",
    "2. **Plateau Effect**: Validation accuracy stops improving while training accuracy keeps rising\n",
    "3. **Noise Learning**: Deep trees fit training noise rather than true patterns\n",
    "\n",
    "### Strategies to Reduce Overfitting\n",
    "\n",
    "- **Limit max_depth**: Prevent trees from growing too complex\n",
    "- **Increase min_samples_split**: Require more samples to create splits (reduces tree complexity)\n",
    "- **Pruning**: Remove branches that don't significantly improve validation accuracy\n",
    "- **Use validation data**: Select hyperparameters based on validation performance, not training\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The results from `max_depth_analysis()` demonstrate the bias-variance tradeoff. The optimal model depth is where validation accuracy is highest. Using deeper trees may achieve higher training accuracy but sacrifices generalization, indicating overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f2687",
   "metadata": {},
   "source": [
    "# Part D: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78918ad",
   "metadata": {},
   "source": [
    "## Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "604139e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self,min_sample_split=2 , max_depth=4 , n_trees=10, n_features=None):\n",
    "        self.min_sample_split=min_sample_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_trees=n_trees\n",
    "        self.n_features=n_features\n",
    "        self.trees=[]\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.trees=[]\n",
    "        for _ in range(self.n_trees):\n",
    "            tree= DecisionTree(min_samples_split=self.min_sample_split, max_depth=self.max_depth, max_features=self.n_features)\n",
    "            X_bt, y_bt=self._bootstrap_sample(X,y)\n",
    "            tree.fit(X_bt,y_bt)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def _bootstrap_sample(self,X,y):\n",
    "        n_samples=X.shape[0]\n",
    "        indices=np.random.choice(n_samples,size=n_samples,replace=True)\n",
    "        return X[indices], y[indices]\n",
    "    \n",
    "    def predict(self,x):\n",
    "        predictions=np.array([tree.predict(x) for tree in self.trees])\n",
    "        majority_votes=[np.bincount(pred).argmax() for pred in predictions.T]\n",
    "        return np.array(majority_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a1671",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "225875b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_tuning(X_train, y_train, X_val, y_val, d):\n",
    "    T_values = [5, 10, 30, 50]\n",
    "    max_features_values = [int(np.sqrt(d)), int(d/2)]\n",
    "    best_acc = -1\n",
    "    best_params = None\n",
    "    results = []\n",
    "    print(\"\\n===== Random Forest Validation Results =====\")\n",
    "    print(f\"{'T':<10} | {'max_features':<15} | {'Val Accuracy':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    for T in T_values:\n",
    "        for mf in max_features_values:\n",
    "            rf = RandomForest(\n",
    "                n_trees=T,\n",
    "                n_features=mf,\n",
    "                max_depth=4,            \n",
    "                min_sample_split=2  \n",
    "            )\t\t\n",
    "            rf.fit(X_train, y_train)\n",
    "            preds = rf.predict(X_val)\n",
    "            acc = accuracy_score(y_val, preds)\n",
    "\n",
    "            print(f\"{T:<10} | {mf:<15} | {acc:.4f}\")\n",
    "\n",
    "            results.append((T, mf, acc))\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = (T, mf)\n",
    "\n",
    "    print(\"\\nBEST RANDOM FOREST PARAMETERS:\")\n",
    "    print(f\"T = {best_params[0]}, max_features = {best_params[1]}\")\n",
    "    print(f\"Validation Accuracy = {best_acc:.4f}\")\n",
    "\n",
    "    return best_params, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3cc56",
   "metadata": {},
   "source": [
    "## best (T, max_features) combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ddbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest Validation Results =====\n",
      "T          | max_features    | Val Accuracy\n",
      "---------------------------------------------\n",
      "5          | 5               | 0.9765\n",
      "5          | 15              | 0.9647\n",
      "10         | 5               | 0.9647\n",
      "10         | 15              | 0.9765\n",
      "30         | 5               | 0.9882\n",
      "30         | 15              | 0.9882\n",
      "50         | 5               | 0.9765\n",
      "50         | 15              | 0.9882\n",
      "\n",
      "BEST RANDOM FOREST PARAMETERS:\n",
      "T = 30, max_features = 5\n",
      "Validation Accuracy = 0.9882\n"
     ]
    }
   ],
   "source": [
    "d = X_train.shape[1]   # number of features\n",
    "\n",
    "# Step 1 — tune\n",
    "best_params, _ = random_forest_tuning(X_train, y_train, X_val, y_val, d)\n",
    "best_T, best_mf = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3460e28",
   "metadata": {},
   "source": [
    "## Retrain the random forest on training + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64e508e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== TRAINING FINAL RANDOM FOREST MODEL ====\n",
      "\n",
      "==== FINAL TEST PERFORMANCE ====\n",
      "===== PERFORMANCE METRICS =====\n",
      "Accuracy:  0.9070\n",
      "Precision: [0.92857143 0.89655172]\n",
      "Recall:    [0.8125     0.96296296]\n",
      "F1-score:  [0.86666667 0.92857143]\n",
      "\n",
      "===== CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9286    0.8125    0.8667        32\n",
      "           1     0.8966    0.9630    0.9286        54\n",
      "\n",
      "    accuracy                         0.9070        86\n",
      "   macro avg     0.9126    0.8877    0.8976        86\n",
      "weighted avg     0.9085    0.9070    0.9055        86\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[26  6]\n",
      " [ 2 52]]\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — combine train and val sets\n",
    "X_train_val, y_train_val = train_val_combine(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# train final model\n",
    "final_rf = RandomForest(\n",
    "    n_trees=best_T,\n",
    "    n_features=best_mf,\n",
    "    max_depth=4,\n",
    "    min_sample_split=2\n",
    ")\n",
    "print(\"\\n==== TRAINING FINAL RANDOM FOREST MODEL ====\")\n",
    "final_rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# test\n",
    "\n",
    "test_preds = final_rf.predict(X_test)\n",
    "\n",
    "print(\"\\n==== FINAL TEST PERFORMANCE ====\")\n",
    "evaluate_performance(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5b9da",
   "metadata": {},
   "source": [
    "# compare the performance of a single decision tree with the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88b087d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== DECISION TREE TEST PERFORMANCE ====\n",
      "\n",
      "===== PERFORMANCE METRICS =====\n",
      "Accuracy:  0.8837\n",
      "Precision: [0.86666667 0.89285714]\n",
      "Recall:    [0.8125     0.92592593]\n",
      "F1-score:  [0.83870968 0.90909091]\n",
      "\n",
      "===== CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8667    0.8125    0.8387        32\n",
      "           1     0.8929    0.9259    0.9091        54\n",
      "\n",
      "    accuracy                         0.8837        86\n",
      "   macro avg     0.8798    0.8692    0.8739        86\n",
      "weighted avg     0.8831    0.8837    0.8829        86\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[26  6]\n",
      " [ 4 50]]\n",
      "\n",
      "\n",
      "\n",
      "==== FINAL RANDOM FOREST TEST PERFORMANCE ====\n",
      "\n",
      "===== PERFORMANCE METRICS =====\n",
      "Accuracy:  0.9070\n",
      "Precision: [0.92857143 0.89655172]\n",
      "Recall:    [0.8125     0.96296296]\n",
      "F1-score:  [0.86666667 0.92857143]\n",
      "\n",
      "===== CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9286    0.8125    0.8667        32\n",
      "           1     0.8966    0.9630    0.9286        54\n",
      "\n",
      "    accuracy                         0.9070        86\n",
      "   macro avg     0.9126    0.8877    0.8976        86\n",
      "weighted avg     0.9085    0.9070    0.9055        86\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[26  6]\n",
      " [ 2 52]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== DECISION TREE TEST PERFORMANCE ====\\n\")\n",
    "\n",
    "evaluate_performance(y_test, test_preds_trees)\n",
    "\n",
    "print(\"\\n\\n\\n==== FINAL RANDOM FOREST TEST PERFORMANCE ====\\n\")\n",
    "evaluate_performance(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450deb62",
   "metadata": {},
   "source": [
    "Model Performance (Accuracy & Generalization)\n",
    "### Single Decision Tree\n",
    "\n",
    "Learns simple to very complex patterns depending on depth.\n",
    "\n",
    "Can achieve high training accuracy.\n",
    "\n",
    "Often overfits the training data.\n",
    "\n",
    "Performance on unseen (test) data is usually unstable.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Combines many decision trees built on:\n",
    "\n",
    "Different bootstrap samples (bagging).\n",
    "\n",
    "Different random subsets of features.\n",
    "\n",
    "Predictions are made by majority voting (classification) or averaging (regression).\n",
    "\n",
    "Achieves higher and more stable test accuracy than a single tree.\n",
    "\n",
    "Much better generalization.\n",
    "\n",
    "### Conclusion on Performance:\n",
    "Random Forest almost always outperforms a single decision tree on real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eae995",
   "metadata": {},
   "source": [
    "## discuss the effect on bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6499023",
   "metadata": {},
   "source": [
    "### Effect on bias\n",
    "\n",
    "| Model             | Bias Level                               | Explanation                                                                 |\n",
    "| ----------------- | ---------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| **Decision Tree** | **Low Bias**                             | It can fit very complex patterns, especially when deep.                     |\n",
    "| **Random Forest** | **Slightly Higher Bias (but still low)** | Each tree is slightly restricted by randomness, but still flexible overall. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648ed15",
   "metadata": {},
   "source": [
    "### Effect on variance\n",
    "\n",
    "| Model             | Variance Level           | Explanation                                                |\n",
    "| ----------------- | ------------------------ | ---------------------------------------------------------- |\n",
    "| **Decision Tree** |  **Very High Variance** | Small data changes can produce completely different trees. |\n",
    "| **Random Forest** |  **Low Variance**       | Averaging many trees cancels out individual errors.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affd052",
   "metadata": {},
   "source": [
    "### Bias–Variance Tradeoff Summary\n",
    "\n",
    "| Model                    | Bias       | Variance | Overfitting | Stability     |\n",
    "| ------------------------ | ---------- | -------- | ----------- | ------------- |\n",
    "| **Single Decision Tree** | Low        |  High   |  High      |  Unstable    |\n",
    "| **Random Forest**        | Low–Medium |  Low    |  Low       |  Very Stable |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
