{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27293643",
   "metadata": {},
   "source": [
    "## Decision Trees Implementation ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec28c7c",
   "metadata": {},
   "source": [
    "# 1) Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "##### Data Setup ######\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_breast_cancer()\n",
    "X = data.data           \n",
    "y = data.target        \n",
    "feature_names = data.feature_names  \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split the remaining 30% into 15% val and 15% test\n",
    "# 15% is half of 30% → test_size = 0.5\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a6bdd",
   "metadata": {},
   "source": [
    "## 2)Node Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        if self.value is not None:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f587fb",
   "metadata": {},
   "source": [
    "## 3) Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1896e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.root=None\n",
    "        self.all_gains=[]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.all_gains = [0] * X.shape[1]\n",
    "        self.root = self._grow_tree(X, y)\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        if(depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        left_mask=X[:, best_feature] <= best_threshold\n",
    "        right_mask=X[:, best_feature] > best_threshold\n",
    "        left_data, right_data = X[left_mask], X[right_mask]\n",
    "        left_labels, right_labels = y[left_mask], y[right_mask]\n",
    "        left_child=self._grow_tree(left_data, left_labels, depth+1)\n",
    "        right_child=self._grow_tree(right_data, right_labels, depth+1)\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "    #### Helper Functions ####\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_overall_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "\n",
    "\n",
    "        for feature in range(n_features):\n",
    "\n",
    "            # Compute all possible midpoints for this feature\n",
    "            values = np.sort(np.unique(X[:, feature]))\n",
    "            thresholds = [(values[i] + values[i+1]) / 2 for i in range(len(values) - 1)]\n",
    "\n",
    "            best_feature_gain = -1  # reset best gain for THIS feature\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                gain = self._info_gain(y, X[:, feature], threshold)\n",
    "\n",
    "                # Update best-overall split\n",
    "                if gain > best_overall_gain:\n",
    "                    best_overall_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "                # Update best gain for THIS feature\n",
    "                if gain > best_feature_gain:\n",
    "                    best_feature_gain = gain\n",
    "\n",
    "            # Save best gain of this feature for feature ranking\n",
    "            self.all_gains[feature] = best_feature_gain\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _info_gain(self, y, feature_column, threshold):\n",
    "        parent_entropy=self.entropy(y)\n",
    "        left_mask=feature_column <= threshold\n",
    "        right_mask=feature_column > threshold\n",
    "        left_y=y[left_mask]\n",
    "        right_y=y[right_mask]\n",
    "        if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:\n",
    "            return 0\n",
    "        left_entropy=self.entropy(left_y)\n",
    "        right_entropy=self.entropy(right_y)\n",
    "        weighted_entropy=(len(left_y)/len(y))*left_entropy + (len(right_y)/len(y))*right_entropy\n",
    "        info_gain=parent_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "    def entropy(self, y):\n",
    "        class_labels, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
    "        return entropy\n",
    "    def _most_common_label(self, y):\n",
    "        class_labels, counts = np.unique(y, return_counts=True)\n",
    "        most_common = class_labels[np.argmax(counts)]\n",
    "        return most_common\n",
    "    def predict(self, X):\n",
    "        root=self.root\n",
    "        predictions=[]\n",
    "        for x in X:\n",
    "            node = self.root\n",
    "            while not node.is_leaf_node():\n",
    "                if x[node.feature] <= node.threshold:\n",
    "                    node=node.left\n",
    "                else:\n",
    "                    node=node.right\n",
    "            predictions.append(node.value)\n",
    "        return np.array(predictions)\n",
    "    def ranked_features(self):\n",
    "       \n",
    "        if not self.all_gains:\n",
    "            return 0\n",
    "        gains=np.argsort(self.all_gains)[::-1]\n",
    "        table = pd.DataFrame({\n",
    "        \"Feature\": [feature_names[i] for i in gains],\n",
    "        \"Information Gain\": [self.all_gains[i] for i in gains]\n",
    "        })\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9325a8",
   "metadata": {},
   "source": [
    "## 4) Analytics Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c12e2f",
   "metadata": {},
   "source": [
    "## 4.1) *Hyperparameter Testing*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Analytics Functions ##########\n",
    "def hyperparameter_tuning(X_train,y_train,X_val,y_val):\n",
    "    best_acc=0\n",
    "    best_params = None\n",
    "    accuracy=[]\n",
    "    best_tree=None\n",
    "    max_depth={2, 4, 6, 8, 10} \n",
    "    min_samples_split = {2, 5, 10}\n",
    "   \n",
    "    results = []\n",
    "\n",
    "    for depth in max_depth:\n",
    "        for min_split in min_samples_split:\n",
    "            tree = DecisionTree(max_depth=depth, min_samples_split=min_split)\n",
    "            tree.fit(X_train, y_train)\n",
    "            preds = tree.predict(X_val)\n",
    "            acc = np.mean(preds == y_val)\n",
    "            print(f\"Depth={depth}, MinSplit={min_split}, ValAcc={acc:.4f}\")\n",
    "            results.append((depth, min_split, acc))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = (depth, min_split)\n",
    "         \n",
    "    print(\"\\nBest Parameters:\", best_params, \"Validation Accuracy:\", best_acc)\n",
    "    return best_params, results\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e243d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, tuning_results = hyperparameter_tuning(X_train, y_train, X_val, y_val)\n",
    "best_depth, best_min_split = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1f1df",
   "metadata": {},
   "source": [
    "## 4.2) *Changing only Max Depth*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_depth_analysis(X_train,y_train,X_val,y_val):\n",
    "    max_depth={2, 4, 6, 8, 10} \n",
    "    min_samples_split=2\n",
    "    table = []\n",
    "    for depth in max_depth:\n",
    "        tree = DecisionTree(max_depth=depth, min_samples_split=min_samples_split)\n",
    "        tree.fit(X_train, y_train)\n",
    "        preds = tree.predict(X_val)\n",
    "        val_acc = np.mean(preds == y_val)\n",
    "        train_preds = tree.predict(X_train)\n",
    "        train_acc = np.mean(train_preds == y_train)\n",
    "        print(f\"Depth={depth}, ValAcc={val_acc:.4f}\")\n",
    "        \n",
    "        table.append((depth, train_acc, val_acc))\n",
    "\n",
    "    print(\"\\nDepth | Train Acc | Val Acc\")\n",
    "    for row in table:\n",
    "        print(f\"{row[0]:5} | {row[1]:9.4f} | {row[2]:8.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_analysis(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74852f6d",
   "metadata": {},
   "source": [
    "##  4.3) *Tree Complexity and Model Performance*\n",
    "\n",
    "To evaluate the effect of model complexity, we analyze how varying the **maximum depth** of the tree influences performance.\n",
    "\n",
    "We examine:\n",
    "- **Training Accuracy**\n",
    "- **Validation Accuracy**\n",
    "- **Overfitting / underfitting behavior**\n",
    "\n",
    "**What we expect:**\n",
    "- At **low depth**, the model underfits → high bias, low accuracy.\n",
    "- As depth increases, training accuracy rises.\n",
    "- Validation accuracy improves at first, then declines when the tree becomes too complex.\n",
    "  This indicates **overfitting**.\n",
    "\n",
    "**How results are presented:**\n",
    "- A table or plot showing accuracy vs. tree depth.\n",
    "- The optimal depth is chosen where validation accuracy peaks.\n",
    "- This depth is then used to retrain on training + validation data and evaluated on the test set.\n",
    "\n",
    "**Conclusion:**\n",
    "- Tree depth strongly affects generalization.\n",
    "- The best performance occurs at the depth that balances bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26dd56",
   "metadata": {},
   "source": [
    "## 4.4) *Accuarcy measures on Test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_combine(X_train, y_train, X_val, y_val):\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "    y_train_val = np.hstack((y_train, y_val))\n",
    "    return X_train_val, y_train_val\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    print(\"===== PERFORMANCE METRICS =====\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred, average=None)}\")\n",
    "    print(f\"Recall:    {recall_score(y_true, y_pred, average=None)}\")\n",
    "    print(f\"F1-score:  {f1_score(y_true, y_pred, average=None)}\\n\")\n",
    "\n",
    "    print(\"===== CLASSIFICATION REPORT =====\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"===== CONFUSION MATRIX =====\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree = DecisionTree(max_depth=best_depth, min_samples_split=best_min_split)\n",
    "X_train_val, y_train_val = train_val_combine(X_train, y_train, X_val, y_val)\n",
    "max_depth_analysis(X_train, y_train, X_val, y_val)\n",
    "final_tree.fit(X_train_val, y_train_val)\n",
    "test_preds = final_tree.predict(X_test)\n",
    "evaluate_performance(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081078d",
   "metadata": {},
   "source": [
    "## 4.5) *Ranking of Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67364f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree.ranked_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd857bc",
   "metadata": {},
   "source": [
    "## 4.6) *Overfitting Analysis: Training vs. Validation Performance*\n",
    "\n",
    "### Overview\n",
    "Overfitting occurs when a model learns the training data too well, including its noise and peculiarities, resulting in poor generalization to unseen data. For decision trees, this is controlled primarily by **tree depth** and **minimum samples per split**.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "**Training vs. Validation Accuracy Gap:**\n",
    "- As tree depth increases, training accuracy typically continues to improve (approaching 100%)\n",
    "- Validation accuracy initially improves but then plateaus or decreases\n",
    "- The gap between training and validation accuracy indicates overfitting\n",
    "\n",
    "**Optimal Depth:**\n",
    "- At shallow depths (depth=2-4), both accuracies are lower → **underfitting** (high bias)\n",
    "- At moderate depths (depth=6-8), validation accuracy peaks → **balanced model**\n",
    "- At deep depths (depth=10+), large gap appears → **overfitting** (high variance)\n",
    "\n",
    "### How to Identify Overfitting\n",
    "\n",
    "1. **Growing Gap**: If `Training Accuracy - Validation Accuracy > 0.05`, the model is likely overfitting\n",
    "2. **Plateau Effect**: Validation accuracy stops improving while training accuracy keeps rising\n",
    "3. **Noise Learning**: Deep trees fit training noise rather than true patterns\n",
    "\n",
    "### Strategies to Reduce Overfitting\n",
    "\n",
    "- **Limit max_depth**: Prevent trees from growing too complex\n",
    "- **Increase min_samples_split**: Require more samples to create splits (reduces tree complexity)\n",
    "- **Pruning**: Remove branches that don't significantly improve validation accuracy\n",
    "- **Use validation data**: Select hyperparameters based on validation performance, not training\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The results from `max_depth_analysis()` demonstrate the bias-variance tradeoff. The optimal model depth is where validation accuracy is highest. Using deeper trees may achieve higher training accuracy but sacrifices generalization, indicating overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
